from constants import title
from dash import dcc, html, register_page

register_page(__name__, title=f'{title} - About')

layout = html.Div([
    html.H3('Technical Write-Up and Retrospective'),
    html.Hr(style={'margin': '32px 0'}),
    dcc.Markdown('''
        #### Introduction
        
        Data science has an important role to play in how we understand climate change, model its current and future effects, and communicate its impacts. With this project I wanted to explore climate datasets and engage in a data science exercise to draw and communicate climate change-related conclusions while gaining experience pertinent to the career paths I wish to pursue. The general technical challenges ended up being twofold: identifying the appropriate tools to fulfil this goal and learning the necessary skills needed to employ those tools. I decided to tackle these challenges in three steps. First, I would work on analyzing data using the Python programming language along with some data-centric software libraries, then I would use the same tools to visualize that analysis, and finally I wanted to tie everything up with a tidy web application to act as a data dashboard.
        
        Why Python? Python is widely used for data science due to its ease of use, active community, and the availability of many powerful libraries specific to data manipulation, visualization, and other pertinent uses. As a general programming language, Python can build applications and set up websites, among countless other uses, which were two subtasks of my overall goal. Alternatives to Python include R, which is also widely used for data science but narrower in scope, programs such as MATLAB, which can be cost-prohibitive, and similar general languages such as C++ or Java, which don’t have the ease of use that Python enjoys for the tasks at hand.
        
        In preparation, I repurposed an old website and domain name as an entry point to various aspects of the project. I also decided to use git, a version control system, to track changes to code and other documents. I chose GitHub as the repository (or repo) to store these files and keep a history of their revisions. I was able to deploy various aspects of the project straight from GitHub, including the web application, summary website, and Jupyter notebooks. Finally, for general development I relied heavily on PyCharm, a professional-grade integrated development environment (IDE) designed specifically for Python.
        
        #### Datasets
        
        I was pleased to find many climate data sets available online. Many of these came in the form of a comma-separated values (CSV) file, which made them easy to import into my Python program and work with. In many cases, I could simply import the data directly via its web link, though I did end up downloading the files I used in the final project to help it load faster. In all cases I challenged myself to use the CSV files as they were, with any data manipulation done within Python instead of editing the files manually to a desirable state, including operations such as discarding bad data, sorting and changing columns, and merging files together, to name a few.
        
        After reviewing many datasets, I decided to use data from two tide gauge sources that supported my GEOL 330 paper topic, sea-level rise along the California coast. The first was the University of Hawaii Sea Level Center (UHSLC), which has hourly and daily data from various locations worldwide, including along the California coast. Though interesting, I found the data to be too busy for simple visualization. The second source was the Permanent Service for Mean Sea Level (PSMSL), which had similar datasets but at a monthly or annual recurrence. This proved to be much easier to produce meaningful plots with and became the data source for the bulk of my analysis and visualization work. Other datasets were considered in support of this topic, such as temperature increases or ice melt rates, but were ultimately not analyzed. A small plot about CO2 concentrations did make it into the final project, however.
        
        #### Data Analysis
        
        I found that the easiest way to work on data analysis in a stepwise, illustrative fashion was to use a Jupyter notebook. This tool allows the user to write Python code in sections, each of which can be launched independently with the output displayed in line with the code itself.  This made trouble-shooting data manipulation and equations much easier than writing and debugging whole (or partial) programs. Notebooks could be deployed independently or as part of Jupyter’s larger development platform, JupyterLab. I found the latter to be overly complicated for my needs and fell back to using the stand-alone notebooks for much of my work. I also discovered that Google had its own version of this concept called Colab, which appeared to be a great way to do similar work collaboratively. Colab requires its users to log into Google, however, which was a requirement I didn’t want to deal with.
        
        Since notebooks acted as Python environments, it was easy to directly import any sort of Python library to use within. The library that I relied upon the most was pandas (a name derived from the phrase panel data), which is a well-known, widely used library for data analysis and manipulation. This library was used for nearly all my data work aside from visualization, including importing two-dimensional data frames from CSV files and manipulating those data frames for analysis and visualization. The only other library that was used at this stage was statsmodels, which I used for linear regression modelling.
        
        #### Data Visualization
        
        In addition to using notebooks for data analysis, I also began using them to create plots. This is the logical workflow of a notebook, after all. Early on I used a standard Python library called Matplotlib and its submodule pyplot, which features plotting elements like MATLAB. This was a simple entry point into plotting the pandas data frames and validating the data manipulation I was attempting.
        
        I wanted to summarize my data visualization with an interactive dashboard, so I began looking at alternatives to Matplotlib. One that I worked with quite a bit was hvplot, which uses the interactive visualization library bokeh and integrated very well with pandas. This allowed me to easily create some very nice-looking plots. Furthermore, it worked very well with a nice data dashboard tool called Panel that integrated well with the notebooks. I ultimately decided to move away from this setup since I did not intend to use a notebook for my final project and wanted to mitigate the number of new tools I had to learn. I eventually settled on Dash, a dashboard tool from Plotly, which also makes a data visualization library of the same name. Since Dash was built to work with plotly, and plotly worked in notebooks, I committed to learning this tool instead.
        
        #### Sharing Data Analysis and Visualization        
        
        Sharing the notebooks turned out to be a non-trivial task. There were several options to start up a server on which to deploy live notebook instantiations, but this turned out to be ill-advised due to the inclusion of a terminal (with which ne'er-do-wells could cause mischief with the server itself). Instead, I attempted to find a more turn-key solution that wouldn’t have these risks. The simplest was uploading notebooks to GitHub, which allowed a rudimentary version of a notebook to be viewed at a direct link but did not allow anything to be run or edited. Since I wanted to feature interactive plots, this wasn’t an ideal solution. Furthermore, GitHub couldn’t display the plotly graphs, which are embedded HTML/JavaScript elements rather than static images.
        
        I discovered two alternatives within Jupyter’s larger collection of products: nbviewer and binder. Both tools worked by providing a link to a notebook’s GitHub repository, from which the notebook is copied and deployed in a temporary environment. Nbviewer’s environment was static – that is, it was not dissimilar to the GitHub option. However, it did display all the elements the way they were meant to be seen, including the plotly graphs, so was a step-up in that regard. Binder worked much the same way as nbviewer except that it launched a dedicated, executable environment in which one could edit and execute code as if in a normal JupyterLab instance without effecting the original files.
        
        #### Web Application
        
        The final part of this project was to develop a simple web application to succinctly summarize and display the analysis results. I considered building a dedicated website for this and investigated Python web frameworks such as Flask or Django. I eventually discovered Dash, which is a data dashboard framework that is built upon Flask. Since Dash used plotly for its visual elements, which I was also able to use in the notebooks, it was ideal as common solution. Its structure was not too dissimilar from Flask, which I was already familiar with, and I could move the plotly graphs from notebooks with less hassle than if I had to port them to a different plotting library.
        
        There are many options these days for quickly deploying web applications. Among the most popular are Heroku, Digital Ocean, and PythonAnywhere, which is specific to Python apps. Though these are all comparable in features and cost, I decided to go with Digital Ocean for no other reason other than I had used them in the past and was satisfied with their services. Editing and deploying the application was quite simple once set up: I made my edits in PyCharm, which I could view on my computer as I went, then committed the edits to GitHub whenever they were in a good state. Digital Ocean, which was linked to the GitHub repository, would then rebuild the application and deploy the new version automatically.
        
        #### Conclusion
        
        This was a challenging project for many reasons. Exploring all the available datasets quickly became overwhelming and I kept finding a need to narrow the scope of my analysis. On top of that, Python being a very popular tool meant that there were numerous options available to fulfil the same purposes. Investigating those that seemed to suit my needs and integrate well with each other was a challenge in and of itself. Despite my previous experience, learning to use said tools adequately was not a trivial manner, and I continue to learn more every time I revisit the project.  However, the challenge of such work is one of the main reasons I enjoy it and this exercise has been a fulfilling experience that yielded some interesting conclusions.
    ''', link_target="_blank", className="markdown about")
])
